auth_enabled: false
#search_enabled: true
#metrics_generator_enabled: true
server:
  http_listen_port: 3200
distributor:
  receivers:                           # this configuration will listen on all ports and protocols that tempo is capable of.
    jaeger:                            # the receives all come from the OpenTelemetry collector.  more configuration information can
      protocols:                       # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver
        thrift_http:                   #
        grpc:                          # for a production deployment you should only enable the receivers you need!
        thrift_binary:
        thrift_compact:
    zipkin:
    otlp:
      protocols:
        http:
        grpc:
    opencensus:
ingester:
  trace_idle_period: 10s               # the length of time after a trace has not received spans to consider it complete and flush it
  max_block_bytes: 524288000           # cut the head block when it hits this size or ...
  max_block_duration: 30m               #   this much time passes
compactor:
  compaction:
    compaction_window: 1h              # blocks in this time window will be compacted together
    block_retention: 72h
    compacted_block_retention: 1h
metrics_generator:
  registry:
    external_labels:
      source: tempo
      cluster: kubernetes
  storage:
    path: /tmp/tempo/generator/wal
    remote_write:
      - url: http://prometheus:9090/api/v1/write
        send_exemplars: true
  processor:
    service_graphs:
      dimensions:
        - service.instance.id
        - service.namespace
    span_metrics:
      dimensions:
        - service.instance.id
        - service.namespace
storage:
  trace:
    #cache: redis
    backend: local                     # backend configuration to use
    block:
      bloom_filter_false_positive: .05 # bloom filter false positive rate.  lower values create larger filters but fewer false positives
      v2_encoding: zstd
    wal:
      path: /tmp/tempo/wal             # where to store the wal locally
      v2_encoding: snappy
    local:
      path: /tmp/tempo/blocks
    pool:
      max_workers: 200                 # worker pool determines the number of parallel requests to the object store backend
      queue_depth: 20000
    search:
      cache_control:
        footer: true
        column_index: true
        offset_index: true
    #redis:
      #endpoint: common-redis-master:6379
      #password: D9yLSkckZq
      #timeout: 5000ms
querier:
  trace_by_id:
    query_timeout: 30s
  max_concurrent_queries: 50
overrides:
  metrics_generator_processors: [ "service-graphs", "span-metrics" ]
  max_bytes_per_trace: 15000000